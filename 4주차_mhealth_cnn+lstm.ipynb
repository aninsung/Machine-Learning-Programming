{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "mount_file_id": "https://github.com/aninsung/Machine-Learning-Programming/blob/main/4%EC%A3%BC%EC%B0%A8_mhealth_cnn%2Blstm.ipynb",
      "authorship_tag": "ABX9TyMRI0opquU960Bs/Xa1s146",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aninsung/Machine-Learning-Programming/blob/main/4%EC%A3%BC%EC%B0%A8_mhealth_cnn%2Blstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (오늘 실습과제)\n",
        "\n",
        "Mhealth dataset으로 인간행동 분류하기\n",
        "\n",
        "일단 데이터 읽어서 subject 번호가 컬럼면에 포함된 통합 data frame 만들고 CNN, LSTM, CNN-LSTM 해보기.\n",
        "\n",
        "1. LLM(sLLM)을 제외한 어떤 모델을 써도 가능. 단 라이브러리 import 불가. 모델의 풀 소스가 있어야 함.\n",
        "\n",
        " 2. 10명을 1+2, 3+4, 4+5,.... 5개그룹으로 나눠 2명씩 테스트 데이터로 하면서 cross validation.\n",
        "\n",
        " 3. 데이터 증대는 안됨.\n",
        "\n",
        "4. 오직 모델로만 성능 향상\n",
        "\n",
        "5. 상위 30%씩 차등.\n",
        "\n",
        "6. 성적에 반영\n",
        "\n",
        "7. 11월15일까지. Github게시\n",
        "\n",
        "교차 평가로 평균 F1 기준.\n",
        "\n",
        "8. 본인이 완벽하게 소스 및 모델을 설명할 수 있어야 함. GPT를 쓰던 뭘 쓰던 그건 상관안함. 재현성이 보장되어야 함.\n",
        "--------------------------------------------------------------------------\n",
        "--------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "Kzbk4hbgc2Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터설명\n",
        "📌 Dataset Summary\n",
        "\n",
        "Subjects: 10명 (다양한 프로필의 자원자)\n",
        "\n",
        "Activities: 12개\n",
        "\n",
        "Sensors: 3개 (가슴, 오른쪽 손목, 왼쪽 발목 → elastic strap으로 고정)\n",
        "\n",
        "측정 정보:\n",
        "\n",
        "Acceleration (가속도)\n",
        "\n",
        "Gyroscope (회전율)\n",
        "\n",
        "Magnetometer (자기장 방향)\n",
        "\n",
        "Chest sensor: 2-lead ECG 추가 (활동 인식에는 사용 안 함, 향후 연구용)\n",
        "\n",
        "Sampling Rate: 50 Hz\n",
        "\n",
        "환경: Out-of-lab, 제약 없이 자유롭게 활동 수행 (단, 최선을 다해 실행)\n",
        "\n",
        "📌 Activity Set (12 classes)\n",
        "\n",
        "정적 활동: Standing still, Sitting, Lying down\n",
        "\n",
        "일상/이동 활동: Walking, Climbing stairs, Cycling\n",
        "\n",
        "운동/역동적 활동: Jogging, Running, Jump front & back\n",
        "\n",
        "반복 동작: Waist bends, Arm elevation, Knees bending\n",
        "\n",
        "📌 특징\n",
        "\n",
        "ECG 포함: 운동 중 심전도 기록 → 부정맥, 심박수 모니터링, 운동 효과 분석 가능\n",
        "\n",
        "다양성 확보:\n",
        "\n",
        "동작 강도 차이 (예: Sitting vs. Running)\n",
        "\n",
        "속도 차이 (예: Walking vs. Jogging)\n",
        "\n",
        "다른 신체 부위 동작 (예: Arm elevation vs. Knees bending)\n",
        "\n",
        "실생활 반영: 실제 환경에서 수행되어 일반화 성능 높음"
      ],
      "metadata": {
        "id": "NYZGoj3bciYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "# 1. 파일 리스트\n",
        "files = sorted(glob.glob(\"/content/drive/MyDrive/Colab Notebooks/mhealth+dataset/MHEALTHDATASET/mHealth_subject*.log\"))\n",
        "\n",
        "# 2. 컬럼 정의\n",
        "columns = [\n",
        "    \"chest_acc_x\",\"chest_acc_y\",\"chest_acc_z\",\n",
        "    \"ecg_1\",\"ecg_2\",\n",
        "    \"chest_gyro_x\",\"chest_gyro_y\",\"chest_gyro_z\",\n",
        "    \"chest_mag_x\",\"chest_mag_y\",\"chest_mag_z\",\n",
        "    \"ankle_acc_x\",\"ankle_acc_y\",\"ankle_acc_z\",\n",
        "    \"ankle_gyro_x\",\"ankle_gyro_y\",\"ankle_gyro_z\",\n",
        "    \"ankle_mag_x\",\"ankle_mag_y\",\"ankle_mag_z\",\n",
        "    \"wrist_acc_x\",\"wrist_acc_y\",\"wrist_acc_z\",\n",
        "    \"label\"\n",
        "]\n",
        "\n",
        "# 3. 각 파일 읽어서 subject_id 추가\n",
        "dfs = []\n",
        "for i, file in enumerate(files, start=1):\n",
        "    df = pd.read_csv(file, sep=\"\\s+\", header=None)\n",
        "    df.columns = columns\n",
        "    df[\"subject_id\"] = i\n",
        "    dfs.append(df)\n",
        "\n",
        "# 4. 통합 DataFrame\n",
        "data = pd.concat(dfs, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "D1Aa1xO4LxT5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "outputId": "edebb9c4-f664-4194-cf6f-e88e7f08b13b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:24: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-4155912323.py:24: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  df = pd.read_csv(file, sep=\"\\s+\", header=None)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4155912323.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\s+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subject_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_windows(df, frame_size=100, hop_size=50):\n",
        "    X, y = [], []\n",
        "    data_values = df.drop([\"label\", \"subject_id\"], axis=1).values\n",
        "    labels = df[\"label\"].values\n",
        "    for i in range(0, len(data_values)-frame_size, hop_size):\n",
        "        X.append(data_values[i:i+frame_size])\n",
        "        y.append(labels[i+frame_size-1])  # 마지막 시점 label\n",
        "    return np.array(X), np.array(y)\n"
      ],
      "metadata": {
        "id": "md-2RokFUIAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38726812"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense, Dropout\n",
        "\n",
        "def residual_block(x, filters, kernel_size=3, stride=1):\n",
        "    shortcut = x\n",
        "    # 첫 번째 Conv\n",
        "    x = Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    # 두 번째 Conv\n",
        "    x = Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    # shortcut 차원 맞추기\n",
        "    if shortcut.shape[-1] != filters or stride != 1:\n",
        "        shortcut = Conv1D(filters, 1, strides=stride, padding='same')(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def build_resnet(timesteps, n_features, n_classes):\n",
        "    inputs = Input(shape=(timesteps, n_features))\n",
        "\n",
        "    x = Conv1D(64, 7, strides=2, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    x = residual_block(x, 64)\n",
        "    x = residual_block(x, 128, stride=2)\n",
        "    x = residual_block(x, 256, stride=2)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 결과 저장용 리스트\n",
        "acc_list = []\n",
        "loss_list = []\n",
        "f1_list = []\n",
        "\n",
        "subjects = np.arange(1,11)\n",
        "folds = [(subjects[i], subjects[i+1]) for i in range(0,10,2)]\n",
        "\n",
        "# Define parameters for windowing and model\n",
        "timesteps = 100\n",
        "n_features = data.drop([\"label\", \"subject_id\"], axis=1).shape[1]\n",
        "n_classes = data[\"label\"].nunique()\n",
        "\n",
        "for fold_idx, test_subjects in enumerate(folds, start=1):\n",
        "    # Train/Test 분리\n",
        "    test_data = data[data[\"subject_id\"].isin(test_subjects)]\n",
        "    train_data = data[~data[\"subject_id\"].isin(test_subjects)]\n",
        "\n",
        "    # 윈도우 생성\n",
        "    X_train, y_train = create_windows(train_data, frame_size=timesteps, hop_size=50)\n",
        "    X_test, y_test = create_windows(test_data, frame_size=timesteps, hop_size=50)\n",
        "\n",
        "    print(f\"\\n=== Fold {fold_idx}: Test subjects {test_subjects} ===\")\n",
        "    print(\"Samples:\", X_train.shape[0], \"(train)\", X_test.shape[0], \"(test)\")\n",
        "\n",
        "    # 모델\n",
        "    model = build_resnet(timesteps, n_features, n_classes)\n",
        "    # 학습\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "    # 평가\n",
        "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # F1-score 계산 (다중 클래스는 'macro' 추천)\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='macro')\n",
        "\n",
        "    print(f\"Test Accuracy: {acc:.4f}, Loss: {loss:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "    # 리스트에 저장\n",
        "    acc_list.append(acc)\n",
        "    loss_list.append(loss)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "# Cross-validation 평균\n",
        "print(\"\\n=== Cross-validation Average ===\")\n",
        "print(f\"Average Accuracy: {np.mean(acc_list):.4f}\")\n",
        "print(f\"Average Loss: {np.mean(loss_list):.4f}\")\n",
        "print(f\"Average F1-score: {np.mean(f1_list):.4f}\")"
      ],
      "metadata": {
        "id": "Rzs_f6pL2CmY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}