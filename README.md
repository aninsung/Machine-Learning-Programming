ğŸ“˜ ë”¥ëŸ¬ë‹ ì‘ìš© ê°•ì˜ ì •ë¦¬ (1~11ì£¼ì°¨)
1ì£¼ì°¨

ì‹œí—˜ ê³µì§€

ì¤‘ê°„ê³ ì‚¬: 10ì›” 22ì¼ ì˜¤ì „ 10ì‹œ (ì‹¤ê¸° ì½”ë”© ì‹œí—˜)

ë²”ìœ„: ì¸ê³µì§€ëŠ¥ ê¸°ì´ˆ FAQ í¬í•¨

í•µì‹¬ ê°œë…

Pre-trained model: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ í™œìš©

Semi-supervised learning: ì§€ë„ + ë¹„ì§€ë„ í˜¼í•©

ë¨¸ì‹ ëŸ¬ë‹ vs ë”¥ëŸ¬ë‹ ì°¨ì´

ML: íŠ¹ì§•(Feature) ì¶”ì¶œ í›„ ì…ë ¥

DL: ëª¨ë¸ ë‚´ë¶€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ

Classification vs Regression

Classification: ë²”ì£¼ ë¶„ë¥˜

Regression: ì—°ì†ê°’ ì˜ˆì¸¡

Overfitting vs Underfitting

Overfitting: ë°ì´í„° ê³¼ë‹¤ â†’ ì¡ìŒ í•™ìŠµ

Underfitting: ë°ì´í„° ë¶€ì¡± â†’ í•™ìŠµ ë¶ˆì¶©ë¶„

í•´ê²°ë²•: Feature selection, ê·œì œ(L1/L2)

ë”¥ëŸ¬ë‹ ê¸°ë³¸ êµ¬ì„±ìš”ì†Œ

í™œì„±í™” í•¨ìˆ˜: ë¹„ì„ í˜•ì„± ì¶”ê°€

Optimizer: ê²½ì‚¬í•˜ê°•ë²• ê¸°ë°˜ ìµœì í™”

Backpropagation / Forward propagation

Loss function: CCE, BCE, MSE

One-hot encoding

2ì£¼ì°¨

ëª¨ë¸ ì¼ë°˜í™”: Train / Test ë¶„ë¦¬

ì°¨ì› ì¶•ì†Œ í•„ìš”ì„±: ë°ì´í„° ê³ ì°¨ì› â†’ ê³„ì‚° ë³µì¡, Overfitting ìœ„í—˜

Masked learning: ì¼ë¶€ ê°€ë¦¬ê³  ì˜ˆì¸¡ â†’ ì¼ë°˜í™” ì„±ëŠ¥ â†‘

êµì°¨ ê²€ì¦ (K-fold)

Train/Test ë¶„ë¦¬ ë°˜ë³µ â†’ í‰ê·  ì„±ëŠ¥ í‰ê°€

ë°ì´í„° íŠ¹ì„± ë°˜ì˜ â†’ ì¼ë°˜í™” ê°•í™”

Accuracy vs F1-score

Imbalanced data â†’ Accuracy ë†’ê³  F1 ë‚®ì„ ìˆ˜ ìˆìŒ

ë”°ë¼ì„œ F1-score ì¤‘ìš”

4ì£¼ì°¨

í‰ê°€ ì§€í‘œ

Classification â†’ Accuracy, CCE/BCE

Regression â†’ MSE

CNN vs Dense

CNN: Local feature ì¶”ì¶œ (ê³µê°„ ì •ë³´ ë°˜ì˜)

Dense: Global feature ì¶”ì¶œ

Convolution ì—°ì‚°

Pooling: ì—°ì‚°ëŸ‰ ê°ì†Œ, ì •ë³´ ì¶•ì•½, ì´ë™ ë¶ˆë³€ì„±

RNN/LSTM

RNN: ì´ì „ ìƒíƒœ ê³ ë ¤

LSTM: Vanishing Gradient í•´ê²° (ê²Œì´íŠ¸ êµ¬ì¡°)

GRU: ê³„ì‚° ë‹¨ìˆœí™”

5ì£¼ì°¨

Attention ê°œë…

ë°ì´í„° ì¤‘ ì¤‘ìš”í•œ ë¶€ë¶„ ì§‘ì¤‘

"Attention is all you need"

Seq2Seq ëª¨ë¸

Encoder â†’ Decoder êµ¬ì¡°

ë²ˆì—­ê¸°, ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë³€í™˜

Autoencoder

ë°ì´í„° ì¶•ì•½, Latent vector ì¶”ì¶œ

ì •ìƒ ë°ì´í„° Reconstruction error â†“

ë¹„ì •ìƒ ë°ì´í„° Reconstruction error â†‘

6ì£¼ì°¨

GPTì˜ ê°•ì 

ëŒ€ê·œëª¨ Pre-trained, ë§ì€ íŒŒë¼ë¯¸í„°

Transformer í•µì‹¬

Self-attention, Embedding, Positional encoding, Multi-head attention

Batch Normalization

ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™”(Internal Covariate Shift) ì™„í™”

í•™ìŠµ ì•ˆì •ì„±â†‘, ì†ë„â†‘

7ì£¼ì°¨

CNN ë°œì „ ê³„ì—´

ResNet: Skip connection (Gradient vanishing ë°©ì§€)

Inception: ë‹¤ì–‘í•œ í¬ê¸° í•„í„° ë™ì‹œ ì ìš©

SENet: ì±„ë„ë³„ ì¤‘ìš”ë„ ê³„ì‚° (Channel Attention)

CBAM: Spatial + Channel attention

Transformer â†’ Vision Transformer

ì´ë¯¸ì§€ë„ Embedding í›„ Transformer ì ìš©

CNN(local) + Transformer(global) â†’ ê²°í•© ëª¨ë¸

Batch Normalization vs Layer Normalization

BatchNorm: ë°°ì¹˜ ë‹¨ìœ„ ì •ê·œí™”

LayerNorm: ë ˆì´ì–´ ë‹¨ìœ„ ì •ê·œí™”

9ì£¼ì°¨

í™•ë¥  ëª¨ë¸

Likelihood: ë°ì´í„° ê¸°ë°˜ ë¶„í¬ ì¶”ì •

MLE: ê°€ëŠ¥ë„ ìµœëŒ€í™”

ìƒì„± ëª¨ë¸

VAE: ì ì¬ ê³µê°„(Latent space)ì—ì„œ ë¶„í¬ ì¶”ì •

GAN: Generator vs Discriminator

Diffusion model: ë…¸ì´ì¦ˆ ì œê±° ê¸°ë°˜ í•™ìŠµ

Split Sequence (ìˆœí™˜ ë°ì´í„°)

Sliding window ë°©ì‹

ê·¼ì²˜ ë°ì´í„° ì—°ê´€ì„± í•™ìŠµ

10ì£¼ì°¨

Latent space

ê³ ì°¨ì› ë°ì´í„° â†’ ì €ì°¨ì› íŠ¹ì§• ê³µê°„

PCA, Autoencoderë¡œ ì°¨ì› ì¶•ì†Œ

VAE Loss (ELBO)

Reconstruction error + KL divergence

GAN ë³€í˜•

CGAN, CycleGAN, Pix2Pix

Diffusion Model

ë°ì´í„°ì— ì ì§„ì ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì¶”ê°€ â†’ ì—­ê³¼ì • í•™ìŠµ

11ì£¼ì°¨

Train/Test ë¶„ë¦¬ ì´ìœ : ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸

í‰ê°€ ì§€í‘œ

Accuracy í•œê³„ â†’ Precision, Recall, F1-score, ROC, AUC

Confusion Matrix â†’ ë¶„ë¥˜ ì„±ëŠ¥ ë¶„ì„

ê²½ì‚¬í•˜ê°•ë²• ì¢…ë¥˜

Batch / Stochastic / Mini-batch

ì •ê·œí™” ê¸°ë²•

L1/L2, Dropout, ì¡°ê¸° ì¢…ë£Œ

CNN ëª©ì 

í•©ì„±ê³±: Local feature ì¶”ì¶œ

Pooling: ì°¨ì› ì¶•ì†Œ, ë¶ˆë³€ì„±

RNN, LSTM, GRU ì°¨ì´

RNN: ë‹¨ìˆœ, ì¥ê¸° ì˜ì¡´ì„± ì•½í•¨

LSTM: ê²Œì´íŠ¸ êµ¬ì¡°ë¡œ ì¥ê¸° ê¸°ì–µ ê°€ëŠ¥

GRU: ê°„ì†Œí™”ëœ LSTM

Transformer í•µì‹¬ ìš”ì†Œ

Embedding, Positional Encoding

Self-attention, Multi-head attention

Scaled dot-product attention
